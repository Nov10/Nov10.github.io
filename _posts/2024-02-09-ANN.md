---
title: 인공신경망 - MNIST 인공신경망 구현
author: 문종욱
date: 2024-02-09
category: AI, Artificial Intelligence
layout: post
---

개인 공부 및 고등학교 발표를 위해 작성된 글을 그대로 옮긴 것이며, 이에 따라 문체가 이상하거나 틀린 내용이 있을 수 있습니다.
외부로 재공유는 자제해 주시되, 필요하다면 출처를 남기고 사용해 주시면 감사하겠습니다.
연락 : mon10lur49@gmail.com

Ⅰ. 서론
-------------
‘생각하는’, ‘미래를 예측하는’ 인공지능에 관한 기술은 여전히 개발이 이루어지고 있고, 새로운 인공지능이 끊임없이 나오고 있다. 그림을 그리는 인공지능인 ‘Stable Diffusion’, 글을 써주는 ‘Chat GPT’ 등 여러 인공지능이 대표적인 예시이다. 이들의 발전은 매우 빠른 속도로 이루어지고 있고, 이런 기술이 등장하는 사회에서 프로그래머라면, 심지어 일반인이더라도 이들의 구조를 아는 것이 필요하다.

발표자는 1학년 교과 ‘과학탐구실험’에서 ‘인공지능을 이용한 한반도의 기후 변화 경향성’이라는 주제로, 2학년 교과 ‘지구과학 1’에서 ‘LSTM을 이용한 미래 기온 예측’을 주제로 발표한 적이 있었다. 이들 모두 데이터를 직접 수집하고 분석 및 정제하여 모델을 구축해 결과값을 계산하였으나, ‘과학탐구실험’에서는 단순한 선형 회귀를 이용한 경향성 파악에 그쳤고, ‘지구과학 1’에서는 딥러닝 모델의 한 종류인 LSTM을 사용하여 미래의 기후를 예측하였으나 직접 처음부터 구축한 모델이 아닌 Tensorflow에서 제공하는 도구를 응용한 것이었다.

두 경우 모두 사용 가능한 딥러닝 모델을 직접 구축하지 않았다는 점에서 아쉬운 부분이 많았다. 그렇기에 이들을 바탕으로 하여 직접 딥러닝 모델을 수학적으로 이해 및 구축하고, 딥러닝의 모방 대상인 실제 인간의 뉴런과의 차이점, 유사점은 무엇인지 알아보기로 하였다. 교과 ‘생명과학 1’에서 학습했던 뉴런과 뇌를 비롯한 신경계의 특징과 인공신경망을 서로 비교하면서 이해하면 두 개 모두에 대한 더 깊은 이해가 가능할 것이라고 생각했다.

개인적으로 ‘생명과학 1’에서 뉴런에 대해 배우다 보면, 이처럼 단순히 자극을 서로에게 전달할 뿐인 뉴런들이 어떻게 복잡하게 구성되어 짜여 있기에 인간의 생각을 구성하는지에 대한 궁금증이 계속 있었다. 단순한 뉴런과 복잡한 생각 사이를 연결하는 것은 대체 무엇이고, 아직 정확히 밝혀지지도 않은 인간의 뇌 구조를 인공신경망이 어떤 구조로 모방하는지 또한 항상 궁금해했었다.

이번 기회에 이러한 궁금증을 해결하기 위해 인공신경망과 인공지능에 대한 수학적 이해와 생물학적 뉴런의 비교를 통해 그 특성을 더 잘 이해하고, 이를 바탕으로 사람이 손으로 작성한 필기체로 되어 있는 숫자가 어떤 숫자인지 구분할 수 있는 인공신경망을 Tensorflow와 같은 다른 편리한 도구를 이용하지 않고 직접 구축하기로 하였다.


Ⅱ. 인공신경망
-------------

### 1. 인공신경망

인공신경망(Artificial Neural Network, ANN)은 생물학의 신경망 중 특별히 동물의 중추신경계, 즉 뇌에서 영감을 얻은 알고리즘이다. 인공신경‘망(Network)’의 이름에서 알 수 있듯이, 인공신경망은 여러 개의 노드(node)들이 망(network) 구조로 연결되어 구성된다.

이러한 하나의 노드(node)는 생물의 뉴런에서 영감을 받은 것이다. 하나의 노드는 일반적으로 기계학습에서 사용되는 알고리즘 중 하나인 ‘로지스틱 회귀(Logistic Regression)’로 구성된다. 로지스틱 회귀란 선형 회귀(Linear Regression)를 [^1] 이용하여 데이터가 어떤 범주에 속할지 분류(Classification)하는 알고리즘이다.

선형 회귀는 아래 식으로 표현할 수 있다. 어떠한 입력 데이터 $x$에 대하여 예측값 $z$는,

<br><center> $z=Wx+b$ (식 1) </center><br>

이때 $W$는 가중치(Weight)이고, $b$는 편향도(Bias)이다. 가중치 $W$와 편향도 $b$는 컴퓨터가 기계학습을 통해 계산해나가며 최적의 값을 찾는 대상이므로, 식 1은 $x->z$인 함수로 생각할 수 있다. 이때 이 함수의 형태는 $W$에 $x$를 곱하고 $b$를 더한 일차함수, 즉 직선(line)의 형태로 나타난다. 수학적으로 $W$는 기울기, $b$는 y 절편이라고 생각할 수 있다.

입력 데이터 $x$에 대하여 이 데이터를 ‘얼마나’ 반영할지 결정하기 위해 가중치 $W$를 곱하여 결정하고, 데이터의 전체적인 값의 ‘치우침’을 결정하기 위해 편향도 $b$를 더하여 값 $z$를 계산한다.

![image01](https://github.com/Nov10/nov10.github.io/assets/91333241/4df8f914-a022-46ea-90e8-5b63535aa644)
<center>[그림 1. 선형 회귀]</center><br>

예를 들어, 그림 1과 같이 파란색의 데이터 모음에 대하여 이 데이터의 경향성을 나타내려는 3개의 직선이 있다고 하자. 이들 중 파란색 데이터의 경향성, 즉 증가하는 방향성을 가장 잘 나타내는 직선은 직선 1임이 명백하다. 컴퓨터는 이를 계산하기 위해 가중치와 편향도, 즉 기울기와 y 절편을 아주 조금씩 바꾸어가며 이에 따라 오차가 어느 정도 변하는지를 계산, 즉 수치미분을 이용하여 최적의 기울기와 y 절편을 찾아 나간다.

로지스틱 회귀는 식 1과 함께 아래와 같이 나타낼 수 있다.

<br><center> $y=f(z)$ (식 2) </center><br>

이때 $b$는 프로그래머가 미리 설정한 활성화 함수이다. 활성화함수는 시그모이드(sigmoid) 함수[^2], 렐루(ReLU) 함수[^3] 등 여러 가지가 존재하며, 어떤 활성화함수를 사용할지는 프로그래머가 설정함에 따라 달라진다.

로지스틱 회귀는 식 1을 통해 계산된 $z$의 값을 활성화함수 $f$에 넣고 계산한 후, 그 값을 활성화함수 종류에 따라 값의 범위를 지정하여 최종값의 범주를 정하는 것으로 해결된다.

![image02](https://github.com/Nov10/nov10.github.io/assets/91333241/10385d0e-fc1a-4ba1-a4d6-7cf6ca89007e)
<center>[그림 2. sigmoid 함수]</center>

그림 2는 가장 대표적인 활성화함수인 sigmoid 함수를 나타낸 것이다. sigmoid 함수의 결과값의 범위는 0과 1 사이이다. 예를 들어 sigmoid 함수를 통해 계산된 값이 0.7이고, 다른 하나는 0.2라면, 프로그래머가 미리 설정한 기준점인 0.5를 기준으로 하여 0.7의 경우는 A 범주로 분류하고, 0.2의 경우는 B 범주로 분류할 수 있다.

물론 이는 sigmoid 함수에 한정된 내용이다. 만일 다른 활성화함수를 사용한다면 범주를 구분하는 값을 달리 해야 할 수도 있으며, 이는 프로그래머가 가장 정확도가 높은 모델을 구축하기 위해 직접 시행착오를 통해 알아내야 하는 값일 수도 있다.

이처럼 선형 회귀를 통해 데이터의 경향성을 가장 잘 나타내는 직선을 찾고, 그 직선과 활성화 함수를 이용하여 어떤 범주에 속할지 분류하는 알고리즘을 로지스틱 회귀라고 부른다.

그리고 이러한 로지스틱 회귀를 여러 개 만든 후 이를 연결하는 것으로 가장 기본적인 인공신경망을 만들 수 있다. 이때 로지스틱 회귀가 하나의 노드가 된다. 만일 한 노드의 구조를 다르게 하면 CNN, RNN, LSTM 등의 다른 인공신경망 모델을 구축할 수 있게 된다.

![image03](https://github.com/Nov10/nov10.github.io/assets/91333241/8bd4918f-d41b-4ed8-a4b4-c005c54c0d7a)
<center>[그림 3. 인공신경망의 구조]</center><br>

인공신경망은 그림 3과 같은 구조를 따른다. 인공신경망은 여러 개의 ‘층(layer)’으로 구성되어 있으며, 각 층은 노드들로 구성된다. 데이터는 기본적으로 그림 3의 왼쪽에서 오른쪽으로 흐른다. 처음 데이터는 Input Layer(입력층)를 통해 인공신경망을 통해 들어오고, 그 다음 층인 Hidden Layer(은닉층)에서 계산된 후 최종적으로 Output Layer(출력층)를 통해 출력된다.

예를 들어, 내일 기온을 예측하는 인공신경망을 구축한다고 하자. 현재 기온, 습도, 태양의 높이, 바람의 세기, 기압의 데이터가 내일의 기온을 예측하는 데 필요할 것이다. 이러한 ‘예측하는데 필요한 데이터’를 Input Layer에 통과시킨다. 일반적으로 Input Layer에서는 데이터에 대한 계산이 이루어지지 않는다. Input Layer에 들어간 데이터들은 다음 층인 Hidden Layer로 그대로 이동한다. Hidden Layer를 구성하는 각각의 노드들은 로지스틱 회귀로 이루어져 있다. 즉, 각 노드가 기온, 습도 등의 데이터에 대해 분류를 시행한다. 이러한 결과값들이 종합되어 Ouput Layer를 통해 출력되고, 이를 적절히 가공하는 것으로 내일의 기온을 예측할 수 있다. 이때 Output Layer 또한 로지스틱 회귀로 이루어진다. 즉, Input Layer를 제외한 모든 층에서 로지스틱 회귀가 이루어진다.

이때 Input Layer와 Output Layer는 각각 가장 처음 층과 마지막 층을 의미하기 때문에 여러 개 존재할 수 없으나, Hidden Layer는 층을 여러 개로 구성할 수 있다. 일반적으로 이 층의 수를 늘리면 늘릴수록 인공신경망의 정확도는 올라간다. 이때 이 층을 늘리는 행위를 인공신경망을 더 깊게(deep) 만든다고 하여 이를 ‘딥러닝(deep learning)’이라고 부른다.

그러나 이는 개념적인 구조에 불과하다. 만일 필요하다면 특수한 경우에 한 해 그림 3의 구조를 그대로 따라 인공신경망을 구축할 수 있으나, 이는 코드의 불필요한 부분이 매우 많아지며 속도가 매우 느린 경우가 많다. 그렇기에 실제로 인공신경망을 구축할 때는 위의 구조를 개념적으로 따라 하며, 실제로 각각의 노드들을 만드는 것이 아니라 모든 가중치와 편향도를 묶어 저장하여 계산하는 방식을 사용한다.

실제로 구축할 때는 배열이나 행렬을 이용하여 가중치와 편향도를 따로 저장한다. 노드를 하나하나 직접 구현하여 연결하면 모든 노드를 직접 탐색하며 값을 계산해야 하지만, 배열과 행렬의 특성을 이용하면 한 층에 존재하는 모든 노드에 대한 계산을 한 번에 처리할 수 있게 된다. 이를 통해 더 빠른 연산이 가능해진다.


[^1]: 어떤 데이터 모음의 경향성을 가장 잘 나타내는 직선(line)을 찾는 알고리즘을 ‘선형(linear) 회귀’라 한다.
[^2]: $y={1 \over (1+e^{-x})}$
[^3]: $y=max(0,x)$


### 2. 인공신경망과 뉴런

![image04](https://github.com/Nov10/nov10.github.io/assets/91333241/26380071-5f79-4a2f-bca9-24dfbf77f4ea)
<center>[그림 4. 인공신경망의 노드와 사람의 뉴런]</center><br>

그림 4는 뉴런의 어떤 부분에서 인공신경망이 영향을 받았는지를 보여준다. 뉴런은 그림 4에서 나타나는 가지돌기를 통해 이전 뉴런의 축삭돌기에서 나온 시냅스가 이후 뉴런으로 전달된다. 이때 이 시냅스를 통한 전기적 자극은 뉴런에 저장되었다가 일정한 양을 넘으면 뉴런 내부에서 흥분이 전도되고, 전도된 흥분은 뉴런의 축삭돌기까지 전도되어 시냅스가 분비돼 이후 뉴런에게 자극이 또다시 전달된다.
이는 아래와 같이 비유할 수 있다.

|뉴런|인공신경망|
|------|------|
|시냅스를 통한 자극의 전달|이전 노드의 출력값을 다음 노드의 입력값으로 전달|
|자극이 임계점을 넘지 못하면 활성화되지 않음|입력값의 합이 일정량을 넘지 못하면 범주를 달리함|
|결과값이 시냅스를 통해 다른 뉴런으로 전달|결과값이 다음 노드의 입력값으로 전달|

뉴런에서는 시냅스를 통해 자극이 전달되며, 자극을 전달받은 뉴런은 다음 뉴런으로 자극을 또다시 전달한다. 이를 모방하여 인공신경망의 노드는 이전 노드에서 데이터를 받아 가공을 거친 후 다음 노드에 전달한다.

또한 자극이 임계점을 넘지 못하면 하나의 뉴런 내부에서 흥분이 전도되지 않는데, 이를 모방하여 인공신경망의 노드는 연결된 이전 노드들의 데이터의 총합이 일정량을 넘지 않으면 0(비활성화)을 다음 뉴런으로 전달하고, 일정량을 넘으면 1(활성화)을 전달한다.

이처럼 생물학적 뉴런을 수학적으로 모방하여 인간의 뇌를 모방하고자 하는 것이 인공신경망이다.
그러나 아래와 같은 차이점도 존재한다.

|뉴런|인공신경망|
|------|------|
|층이 존재하지 않음|여러 개의 층의 존재|
|시간에 따른 과정|시간에 독립적인 과정(즉각적인 연산)|
|자극이 강하면 흥분 전도의 주기가 짧아짐|시간에 독립적이므로, 자극이 강하더라도 주기의 변화가 없음|
|자극성 신경전달물질과 억제성 신경전달물질의 존재|하나의 가중치가 이전 노드로부터 들어오는 값을 증폭시킬지 감소시킬지 결정|

실제 인간의 뇌는 뉴런들의 ‘층’이란 것이 존재하지 않는다. 뉴런은 그저 서로 연결되어 있는 것이고, 각 층마다 뉴런들이 획일적으로 연결되어 있지 않지만, 인공신경망은 층을 기반으로 하고 그 층을 구성하는 노드을 이용하여 계산한다는 한다는 차이점이 있다.

또한 인공신경망은 계산이 즉각적이라는 점이 있다. 인간의 뉴런은 하나의 뉴런에서 흥분이 뉴런의 끝에서 끝으로 이동하는 데에도 시간이 소요되며, 다른 뉴런으로 이동할 때도 시간이 소요된다. 그러나 인공신경망은 수학적 알고리즘에 기반을 두고 있으므로, 연산의 속도를 제외한다면, 즉각적인 결과가 나오게 된다. 즉, 인공신경망은 시간에 독립적이기 때문에 실제 인간의 뇌와 차이점이 존재한다.

또한 인간의 뇌에서는 신경전달물질이 자극성과 억제성 2가지가 존재하는데, 인공신경망은 하나의 가중치가 이를 모두 결정한다. 가중치의 값이 작으면 억제의 효과를 내고, 가중치가 크다면 자극(증폭)의 효과를 발생시킨다.

결과적으로, 인공신경망의 노드는 생물학적 뉴런을 수학적으로 모델링한 결과물이다. 인공신경망 기술은 이렇게 모델링된 뉴런, 인공신경망의 노드를 이용하여 궁극적으로는 뇌를 수학적으로 모델링하고 표현한다는 목적을 가지고 있다.

수학적으로 뉴런과 인간의 뇌를 모방하는 과정은 아래와 같다.

![image05](https://github.com/Nov10/nov10.github.io/assets/91333241/800cfcef-bca0-4014-8c8a-1772a83da716)
<center>[그림 5. 간단한 인공신경망]</center><br>

그림 5와 같이 두 개의 노드를 가지는 입력층, 두 개의 노드를 가지는 은닉층, 하나의 노드를 가지는 출력층으로 구성된 인공신경망을 생각해보자. 가장 왼쪽의 두 개의 원은 입력층을, 중간의 두 개의 원은 은닉층을, 가장 오른쪽의 하나의 원은 출력층을 나타낸다. 데이터는 아래와 같은 흐름으로 전달된다.

+ (1) 입력층으로 데이터 전달<br>
  입력층을 구성하는 노드가 2개이므로, 입력층에 전달되는 데이터는 2개이다. 일반적으로 입력층에 들어오는 데이터는 특별한 계산이나 활성화함수를 적용하지 않고 바로 다음 층으로 전달한다. 이는 아래와 같이 나타낼 수 있다.

  두 개의 입력 데이터 $x_1$, $x_2$에 대하여 출력값 $a_{1}^{(1)}$, $a_{2}^{(1)}$은

  <br><center>
  $\begin{pmatrix}
  a_{1}^{(1)} & a_{2}^{(1)}
  \end{pmatrix} =
  \begin{pmatrix}
   x_{1} &x_{2}
  \end{pmatrix}$
  (식 3)
  </center><br>

  로 나타낼 수 있다.

+ (2) 입력층에서 은닉층으로 데이터 전달<br>
  일반적으로 한 층의 노드는 다음 층의 모든 노드와 연결된다. 입력층의 노드가 2개, 은닉층의 노드가 2개이므로, 입력층의 하나의 노드에서 다음 층인 은닉층의 노드로 갈 수 있는 경우는 2가지, 이러한 노드가 2개이므로 총 4개의 가중치가 필요하다. 따라서 가중치는 $W_{11}^{(2)}$, $W_{12}^{(2)}$, $W_{21}^{(2)}$, $W_{22}^{(2)}$ 4개가 필요하다. 예를 들어, $W_{12}^{(2)}$에서 위첨자인 $(2)$는 두 번째 층으로 전달되는 데이터에 곱하는 가중치임을 뜻하고, 아래 첨자인 $12$는 $1$과 $2$를 뜻하는데, 이전 계층의 $2$번 노드에서 다음 계층의 $1$번 노드로 전달되는 데이터에 하는 가중치임을 뜻한다.

  이때 편향도는 4개인 가중치와 달리 단순히 2개만 해도 아무 문제가 없다. 편향도는 이전 노드의 입력값에 곱하거나 나누는 연산을 하는 것이 아닌 그저 덧셈을 수행하는 것이므로 편향도를 2개를 설정하나 4개를 설정하나 결국 하나의 값으로 대체할 수 있다. 따라서 편향도는 $b _{1}^{(2)}$, $b _{2}^{(2)}$ 두 개만 있으면 충분하다.

  위의 과정을 식 3의 값을 이용하면 활성화함수 이전의 선형 회귀 값 $z _{1}^{(2)}$, $z _{2}^{(2)}$는

  <br><center>
  $\begin{pmatrix}
  z_{1}^{(2)} & z_{2}^{(2)}
  \end{pmatrix} =
  \begin{pmatrix}
  a_{1}^{(1)} & a_{2}^{(1)}
  \end{pmatrix}
  \begin{pmatrix}
  W_{11}^{(2)} & W_{12}^{(2)} \\
  W_{21}^{(2)} & W_{22}^{(2)}
  \end{pmatrix}
  +
  \begin{pmatrix}
  b_{1}^{(1)} & b_{2}^{(1)}
  \end{pmatrix}$
  (식 4)
  </center><br>

  로 나타낼 수 있다. 행렬로 나타난 식 4를 풀어서 나타내면

  <br><center>
  $z _{1}^{(2)} =W _{11}^{(2)} a _{1}^{(1)} +W _{12}^{(2)} a _{2}^{(1)} +b _{1}^{(2)}$
  </center><br>

  <br><center>
  $z _{2}^{(2)} =W _{21}^{(2)} a _{1}^{(1)} +W _{22}^{(2)} a _{2}^{(1)} +b _{2}^{(2)}$
  </center><br>

  로 나타낼 수 있다. 위 두 식을 보면 각각의 노드에서 이전 층(입력층)에서 모든 값($a_{1}^{(1)} , a_{2}^{(1)}$)을 전부 갖고 와 가중치를 곱한 후 더하는 총합($W _{11}^{(2)} a _{1}^{(1)} +W _{12}^{(2)} a _{2}^{(1)}$와 $W _{21}^{(2)} a _{1}^{(1)} +W _{22}^{(2)} a _{2}^{(1)}$)을 구하는 것을 볼 수 있다. 이 과정은 뉴런이 다음 뉴런에 자극을 전달하는 과정에서 영감을 얻어 이를 모방한 것이다. 그러나 차이점 또한 존재하는데, 인공신경망은 이전 층의 모든 노드가 다음 층의 하나의 노드에 전부 연결되기 때문에 층이 존재하지 않는 실제 인간의 뇌와 차이가 존재한다고 볼 수 있다.


+ (3) 은닉층의 결과값 계산<br>
  일반적으로 은닉층에서 하나의 노드의 결과값은 식 4를 통해 계산된 선형 회귀 값 $z _{1}^{(2)}$, $z _{2}^{(2)}$을 각각 활성화함수에 넣는 것으로 계산된다. 활성화함수를 $f$라 할 때 은닉층의 결과값 $a _{1}^{(2)}$, $a _{1}^{(2)}$은

  <br><center>
  $a _{1}^{(2)} =f(z _{1}^{(2)} )$
  (식 5-1)
  </center><br>

  <br><center>
  $a _{2}^{(2)} =f(z _{2}^{(2)} )$
  (식 5-2)
  </center><br>

  으로 계산된다. 결과값은 단순히 선형회귀값을 활성화함수를 통해 계산하는 것으로 해결된다.

+ (4) 은닉층에서 출력층으로 데이터 전달<br>
  출력층으로의 데이터 전달도 과정 2와 같이 이루어진다. 그러나 이번에는 노드가 하나이므로, 최종 선형 회귀 값이 하나로 계산된다.
  또한 이전 층인 은닉층의 노드가 2개, 다음 층인 출력층의 노드가 1개이므로 가능한 경우는 2개, 즉 가중치는 2개이다. 또한 출력층의 노드가 1개이므로 가중치는 1개이다.
  위의 과정을 식 5-1과 식 5-2의 값을 이용하면 활성화함수 이전의 선형 회귀 값 $z _{1}^{(3)}$은

  <br><center>
  $\begin{pmatrix}
  z_{1}^{(3)}
  \end{pmatrix} =
  \begin{pmatrix}
  a_{1}^{(2)} & a_{2}^{(2)}
  \end{pmatrix}
  \begin{pmatrix}
  W_{11}^{(3)} \\
  W_{12}^{(3)} 
  \end{pmatrix}
  +
  \begin{pmatrix}
  b_{1}^{(3)}
  \end{pmatrix}$
  (식 6)
  </center><br>

  로 나타낼 수 있다.
  행렬로 나타낸 식 6을 풀어서 나타내면

  <br><center>
  $z _{1}^{(3)} =W _{11}^{(3)} a _{1}^{(2)} +W _{12}^{(3)} a _{2}^{(2)} +b _{1}^{(3)}$
  </center><br>

  로 나타낼 수 있다.

+ (5) 출력층의 결과값 계산<br>
  출력층의 결과값은 과정 3과 같이 출력층 노드의 선형 회귀 값을 활성화함수에 넣는 것으로 계산된다. 활성화함수를 과정 3에서와 같은 $f$라 할 때 출력층의 결과값 $y$는

  <br><center>
  $y=f(z_{1}^{(3)}$) (식 7)
  </center><br>

  로 나타낼 수 있다. 이 값이 입력 데이터 $x_1$, $x_2$에 대한 최종적으로 계산된 결과값 를 나타낸다. 이때 활성화함수에 따라 값의 범위를 지정하는 것으로 를 분류할 수 있다. 이처럼 분류하고자 하는 값이 특정 값을 기준으로 크거나 작음에 따라 분류하는 것은 생물의 뉴런이 이전 뉴런에서 임계값을 넘는 자극을 전달받아야만 자극이 전달된다는 것에서 영감을 받은 것이다.

인공신경망은 위와 같은 5개의 과정과 식 3~7으로 이루어진다. 이때 데이터가 입력층에서 시작하여 출력층까지 도달하는 과정을 데이터를 먹이(feed) 주듯이 앞으로(forward) 나아간다고 하여 ‘feed forward’라고 부른다.

물론 인간의 뉴런, 뇌, 생각은 아직 완벽하게 밝혀지지 않았기 때문에 위의 과정과 식들이 정확히 인간의 뇌를 모방하였다고 할 수는 없으나, 과학이 밝혀낸 것 안에서 최대한 생물의 뉴런을 모방하여 이를 수학적으로 온전히 나타내고자 하는 것이 바로 인공신경망이다.  물론 실제로 사용 가능한 수준의 인공신경망을 구성하는 노드는 위의 경우와 같이 5개 정도가 아니라 매우 많은 수의 노드를 사용하므로, 인공신경망의 층의 수와 각 층을 구성하는 노드의 수를 다르게 설정한다면 위의 과정은 더욱 길고 복잡해질 것이다. 인간의 뇌는 수백억 개의 뉴런으로 구성되는데, 이러한 뇌를 모방하기 위해선 엄청난 양의 노드가 필요할 것이다. 그러나 여전히 기본적인 논리는 같다. 이전 층의 노드에서 계산된 값이 다음 층의 노드로 전달되고, 각 노드는 로지스틱 회귀와 같은 알고리즘을 이용하여 노드의 출력값을 계산한다.

Ⅲ. 인공신경망 학습
-------------
### 1. 손실함수

인공신경망은 인공신경망의 구조만을 구축한다고 하여 완성되는 것이 아니다. 구조를 구축한 후에, 미래의 값을 가장 잘 예측하거나 분류를 가장 잘 수행하도록 하는 가중치와 편향도의 값을 찾아야만 한다. 이러한 과정을 ‘인공신경망을 학습(train)한다’라고 한다.

인공신경망을 학습시킬 때 가장 중요한 것 중 하나는 바로 인공신경망의 정확도를 측정하는 ‘손실 함수(loss function)’이다. 손실함수는 인공신경망의 정확도, 오차, 실제 값과 예측값의 차이와 비슷한 역할을 한다.

입력 데이터 $x_k$에 대한 실제 결과값 $t_k$와 인공신경망의 결과값 $y_k$가 있다고 하자. 만일 입력 데이터와 실제 결과값 사이의 관계를 ‘완벽하게’ 나타낼 수 있는 함수가 있다고 하면, 이 함수는 $x_k->t_k$로 나타난다. 그러나 인공신경망은 $x_k->y_k$로 나타난다. 만일 모든 $k$에 대하여 $t_k$와 $y_k$가 같다면 해당 인공신경망은 100%의 정확도를 보장하는 것이다. 그러나 이러한 경우는 거의 존재하지 않으므로, 인공신경망의 전체적인 오차 정도를 계산하는 것이 중요하다.

입력 데이터 $x_k$에 대한 실제 결과값 $t_k$와 인공신경망의 결과값 $y_k$에 대하여, 절대 오차[^4] $c_k$는 다음과 같이 계산할 수 있다.

<br><center>
$c_k = |t_k-y_k|$
</center><br>

이때 모든 $k$에 대하여 위 식을 계산한 후 평균을 나타내면 아래와 같이 나타낼 수 있다.

<br><center>
$\frac {1}{n} (c_1+c_2+...+c_n)=\frac{1}{n} \sum\limits_{i=1}^n c_i=\frac{1}{n} \sum\limits_{i=1}^n |t_i-y_i|$
</center><br>

위 식은 단순히 존재하는 모든 경우에 대한 절대 오차의 평균을 계산한 것이다. 이를 MAE(Mean Absolute Error)라는 손실함수로 부른다. MAE 손실함수 또한 많이 사용되기는 하나, 절댓값을 계산하는 것보다는 값을 제곱하는 것이 더 학습에 유리하므로[^5] MSE(Mean Squared Error)를 더 많이 사용한다.

<br><center>
$MSE=\frac{1}{n} \sum\limits_{i=1}^n (t_i-y_i)^2$
</center><br>

그러나 MAE와 MSE는 선형 회귀에서 많이 사용되는 손실함수이고, 로지스틱 회귀에서 자주 사용되는 손실함수는 cross-entropy 함수이다. 이는 아래와 같이 계산된다.

<br><center>
$Cross-Entopry=\frac{1}{n} \sum\limits_{i=1}^n t_i\log{y_i}+(1-t_i)\log({1-y_i})$ (식 8)
</center><br>

식 3~7에서 알 수 있듯, 손실함수에서 인공신경망의 결과값 는 모든 가중치와 편향도에 의존한다. 따라서 단 하나의 가중치나 편향도라도 그 값이 바뀐다면 손실함수의 값이 바뀐다. 따라서 가장 작은 오차를 가지는 인공신경망의 가중치와 편향도의 값을 가장 최적의 값으로 바꾸기 위해서는 손실함수를 반드시 고려해야만 한다.

손실함수는 가장 최적의 가중치와 편향도를 찾는데 고려되는 중요한 요소 중 하나이다. 어떻게 모델을 구축하느냐에 따라, 어떤 종류의 데이터를 입력 데이터로 받느냐, 최종적인 인공신경망의 결과값은 어떻게 나올지에 따라 어떤 손실함수를 사용할지 정하는 것은 정확한 인공신경망 구축에 있어 아주 중요한 요소이다.

[^4]:  만일 절댓값을 사용하지 않는다면 양의 오차를 가지는 경우와 음의 오차를 가지는 경우가 만나 서로가 상쇄되므로 정확한 오차 정도를 계산할 수 없다.
[^5]: 제곱을 하면 1보다 큰 오차는 그 값이 더 커지고, 1보다 작은 오차는 그 값이 더 작아지므로 오차를 더 극명하게 나타낼 수 있다.


### 2. 인공신경망의 학습

인공신경망의 학습은 가장 작은 오차를 가지는 인공신경망의 가중치와 편향도의 값을 가장 최적의 값으로 바꾸는 과정을 뜻한다.

예를 들어, 현재 가중치가 2이고 편향도가 1.5인 하나의 노드를 생각해보자. 이때 손실 함수의 값은 0.7이라고 하자.

만일 가중치를 2에서 2.01로 아주 살짝 변화시켰을 때 손실 함수의 값이 0.7에서 0.69로 변했다면, 가중치가 커짐에 따라 손실함수가 감소하였으므로 가중치는 현재 값인 2보다 더 커져야 한다는 것을 뜻한다.

반대로 가중치를 2에서 1.99로 아주 살짝 변화시켰을 때 손실 함수의 값이 0.7에서 0.701로 변했다면, 가중치가 작아짐에 따라 손실함수가 증가하였으므로 가중치는 현재 값인 2보다 더 커져야 한다는 것을 뜻한다.

![image06](https://github.com/Nov10/nov10.github.io/assets/91333241/57963fe4-6770-45fa-998c-033f9c8628a3)
<center>[그림 6. Gradient Descent Algorithm]</center><br>

그림 6은 가중치가 변함에 따라 손실함수의 값이 어떻게 변하는지를 나타내는 그래프이다. 가로축은 가중치 값이고, 세로축은 손실함수의 값이다.

그래프를 보면 최솟값이 존재한다. 그림 6에서 최솟값일 때의 가로축의 값, 즉 최적의 가중치 값을 $W$라고 하자. 그렇다면 현재 가중치는 $W$로 점점 변해가야만 한다.

예를 들어, 현재 가중치 값이 $W$보다 작은, 그림 6에서 나타나는 Starting Point일 때의 가중치라고 해보자. 이때 현재 가중치의 값이 아주 살짝 변할 때 손실함수의 값은 감소한다. 즉, (손실함수의 변화율) / (가중치의 변화율) = 음수 이다. 이때 가중치의 값은 증가해야만 하는데, 이때 값의 변화율은 (손실함수의 변화율) / (가중치의 변화율)의 반대인 양수여야 한다. 이를 수식으로 나타내면 아래와 같다. 현재 가중치 $W$와 손실함수 $E(W,b)$에 대하여 더 나은 가중치 $W'$은

<br><center>
$W'=W-a \frac{\partial E(W,b)}{\partial W}$ (식 9)
</center><br>

더욱 최적의 가중치에 가까운 $W'$값 은 현재 가중치 $W$에 특정한 값을 뺀 값인데, 그 값은 $a$와 $\frac{\partial E(W,b)}{\partial W}$를 곱한 후 부호를 바꾸는 것으로 계산된다. $\frac{\partial E(W,b)}{\partial W}$는 가중치의 값이 아주 살짝 변할 때 손실함수의 값이 얼마나 변하는지를 나타낸다. 만일 이 값이 음수라면(기울기가 음수라면) 가중치는 증가해야만 하고, 양수라면(기울기가 양수라면) 가중치는 감소해야만 한다. 즉, $\frac{\partial E(W,b)}{\partial W}$와 반대 방향으로 가중치는 변해야만 한다. 따라서 $-$를 붙여주는 것이다. 그리고 값을 한 번에 얼마나 변화시킬지를 상수 $a$를 곱하여 결정한다. 이때 이 상수를 ‘학습의 정도’를 결정한다고 하여 ‘학습률(learning rate)’이라고 부른다.

더 나은 편향도 또한 식 9와 유사하게 아래와 같이 계산된다.

<br><center>
$b'=b-a \frac{\partial E(W,b)}{\partial b}$ (식 10)
</center><br>

식 9와 식 10을 여러 번 반복하는 것으로 현재 가중치와 편향도를 가능한 한 최적의 가중치와 편향도에 가깝게 변화시킬 수 있다. ‘가장 작은 오차를 가지는 인공신경망의 가중치와 편향도의 값을 가장 최적의 값으로 바꾸는 과정’은 식 9와 식 10을 통해 계산할 수 있다. 이러한 방식을 변화율에 따라 점점 최적의 값으로 나아가며 손실 함수를 줄인다는 의미에서 ‘경사 하강법 알고리즘(Gradient Descent Algorithm)’이라고 부른다. 

Ⅳ. 인공신경망 구현
-------------
### 1. MNIST

MNIST, 또는 MNIST 데이터베이스는 Modified National Institute of Standards and Technology Database의 약자로, 다량의 손으로 쓴 필기체 형식의 숫자들을 포함하고 있는 데이터베이스이다. 이는 기계학습, 딥러닝에서 모델의 테스트에 가장 많이 사용되고 있는 데이터베이스 중 하나이다.

![image07](https://github.com/Nov10/nov10.github.io/assets/91333241/08ce6f58-41f0-44cf-936f-88f1c6878ec8)
<center>[그림 7. MNIST 데이터베이스 값 중 하나의 데이터]</center><br>

MNIST 데이터베이스는 6만 개의 학습용 데이터와 1만 개의 테스트 데이터로 구성된다. 하나의 데이터는 그림 7과 같은 그림 데이터[^6]와 실제 그 숫자가 무엇인지 정답으로 구성된다. 그림 7은 MNIST 데이터 중 13번째 데이터이며, 실제 숫자는 3이다.

![image08](https://github.com/Nov10/nov10.github.io/assets/91333241/c6b2f5f6-0b7f-4b21-b80c-0e64388e53d0)
<center>[그림 8. MNIST 데이터베이스 값 중 하나의 데이터 일부]</center><br>

그림 8은 그림 7과 똑같은 데이터를 원시 데이터로 나타낸 것이다. 가장 처음(왼쪽 위)의 값은 ‘3’으로 실제 이 필기체 숫자가 어떤 숫자를 나타내는지 뜻하고, 그 뒤의 데이터는 그림 7에서의 그림을 나타낸다. ‘0’은 검은색을, ‘255’는 하얀색을 나타내며, 그 중간의 값은 0과 255중 가까운 값에 따라 진하기가 다른 회색으로 나타난다. 학습 데이터와 테스트 데이터와 모두 같은 형식으로 구성된다.

이때 실제 이 필기체 숫자가 어떤 숫자를 나타내는 데이터는 정답 데이터이고, 그 뒤의 데이터는 학습 데이터이다. 이러한 정답 데이터와 학습 데이터를 인공신경망을 통해 계산하면 사람의 직접 작성해 필기체로 쓰인 숫자가 실제로 어떤 숫자인지 구분하는 인공신경망을 만들어낼 수 있다.

[^6]: 내부적으로는 전부 숫자 배열로 구성된다. 현대 컴퓨터 구조에 따르면 모든 데이터는 숫자이므로, 인공신경망의 입력 데이터는 숫자만이 가능하다.

### 2. MNIST 인공신경망 구현

프로그래밍 언어로는 파이썬(Python)을 사용하였다.
1. 필요한 라이브러리[^7]를 불러온다.

```python
import numpy as np
import matplotlib.pyplot as plt
```

2. 미리 다운받은 학습 데이터와 테스트 데이터를 가져온다.

```python
training_data = np.loadtxt('./mnist_train.csv', delimiter=',', dtype=np.float32)
test_data = np.loadtxt('./mnist_test.csv', delimiter=',', dtype=np.float32)
```

3. 활성화함수로 sigmoid 함수[^8]를 선택하였다. 이를 아래와 같이 정의한다.[^9]

```python
def sigmoid(x):
  return 1 / (1 + np.exp(-x))
```

4. 수치미분을 위한 함수 numerical_derivative를 아래와 같이 정의한다. 수치미분을 사용할 때 $\frac{f(x+h)-f(x)}{h}$보다 $\frac{f(x+h)-f(x-h)}{2h}$를 사용하는 것이 더 정확함이 알려져 있으므로, 계산을 후자의 방식으로 진행한다.[^10] 

```python
def numerical_derivative(f, x):
  delta_x = 1e-4 #h
  grad = np.zeros_like(x) #수치미분한 값(기울기, gradient)   
  it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])
  
  #모든 수에 대해 실행
  while not it.finished:
      idx = it.multi_index
      tmp_val = x[idx]
      
      x[idx] = float(tmp_val) + delta_x    
      fx1 = f(x) #f(x + delta_x)        
      x[idx] = tmp_val - delta_x
      fx2 = f(x) #f(x - delta_x)
      
      grad[idx] = (fx1 - fx2) / (2 * delta_x)

      x[idx] = tmp_val       
      it.iternext()
  return grad
```

5. 인공신경망을 구축한다. 입력층, 은닉층, 출력층으로 총 3개의 층으로 구성하고, 각 층의 노드의 수는 프로그래머 필요에 따라 변경할 수 있도록 설정한다. 모든 가중치와 편향도는 초기값으로 전부 랜덤하게 설정한다.

```python
class NeuralNetwork:
  def __init__(self, input_nodes, hidden_nodes, ouput_nodes):
    self.input_nodes = input_nodes #입력층 노드 수
    self.hidden_nodes = hidden_nodes #은닉층 노드 수
    self.ouput_nodes = ouput_nodes #출력층 노드 수
    
#은닉층 가중치를 랜덤하게 설정
    self.W2 = np.random.rand(self.input_nodes, self.hidden_nodes)
#은닉층 편향도를 랜덤하게 설정
    self.b2 = np.random.rand(self.hidden_nodes)
    
#출력층 가중치를 랜덤하게 설정
    self.W3 = np.random.rand(self.hidden_nodes, self.ouput_nodes)
#출력층 편향도를 랜덤하게 설정
    self.b3 = np.random.rand(self.ouput_nodes)
    
#학습률 정의 10^-4
    self.learning_rate = 1e-4
```

6. 식 3, 4, 5-1, 5-2, 6, 7, 8을 이용하여 feed forward 함수를 정의한다.[^11]

```python
def feed_forward(self):
    delta = 1e-7
        
    #입력층 -> 은닉층
    z1 = np.dot(self.input_data, self.W2) +self.b2 #식 3, 4
    y1 = sigmoid(z1) #식 5-1, 5-2
    #은닉층 -> 출력층
    z2 = np.dot(y1, self.W3) +self.b3 #식 6
    y = sigmoid(z2) #최종 계산값  #식 7
    #최종 손실함수 계산(cross-entropy) #식 8
    return -np.sum((self.target_data*np.log(y+delta)) + ((1-self.target_data)*np.log((1 - y) + delta)) )
```

7. 학습 함수를 정의한다. 정규화[^12]
 과정 후, 과정 4에서 정의한 수치미분 함수를 이용하여 식 9와 식 10에 따라 가중치와 편향도의 값을 조정한다.[^13]

```python
def train(self, training_data):       
  #정규화
  self.target_data = np.zeros(output_nodes) +0.01
  self.target_data[int(training_data[0])] =0.99        
  self.input_data = (training_data[1:] /255.0 *0.99) +0.01
      
  #feed_forward 함수에 대한 수치미분
  f = lambda x : self.feed_forward()
      
  #수치미분을 통한 값 조정
  self.W2 -=self.learning_rate * numerical_derivative(f, self.W2)
  self.b2 -=self.learning_rate * numerical_derivative(f, self.b2)
  self.W3 -=self.learning_rate * numerical_derivative(f, self.W3)
  self.b3 -=self.learning_rate * numerical_derivative(f, self.b3)
```

8. 새롭게 주어진 데이터에 대하여 인공신경망이 값을 계산(예측)하는 함수를 정의한다. feed forward 함수와 구조가 같다. 이때 one-hot encoding[^14] 방식을 이용하여 값을 출력한다.

```python
def predict(self, input_data):
  #입력층 -> 은닉층
  z1 = np.dot(input_data, self.W2) +self.b2
  y1 = sigmoid(z1)
  #은닉층 -> 출력층        
  z2 = np.dot(y1, self.W3) +self.b3
  y = sigmoid(z2)
  
  #one-hot encoding 방식
  predicted_num = np.argmax(y)     
  return predicted_num
```

9. 각 층의 노드 수를 설정한다.  MNIST 데이터에서 하나의 데이터는 총 785개의 숫자로 구성된다. 가장 첫 번째 숫자는 정답이고, 그 이외의 784개의 숫자는 필기체 숫자를 그림으로 나타낸 것이다. 하나의 필기체 숫자는 28 x 28 크기의 그림이다. 따라서 입력층은 784개의 데이터를 받아야 하므로 입력층의 노드 수는 784개이다.

은닉층의 노드 수는 프로그래머가 임의로 설정할 수 있다. 여기선 100개로 설정하였다. 또한 결과값은 0에서 9까지 총 10개의 숫자 중 하나이므로, 출력층은 10개의 노드로 설정한다.

```python
input_nodex = 784
hidden_nodes = 100
output_nodes = 10
```

위와 같이 1~9까지의 과정을 통해 간단하게 인공신경망을 구축할 수 있다. 위의 과정에서 구현한 코드를 조합 및 변형하여 아래와 같이 직접 학습을 진행하였다.

<center>
1. 학습 환경
실행 환경 : Jupyter NotebookCPU : i5 12400F 2.50 GHzRAM : 32.0GB

2. 학습 데이터6만 개의 MNIST 학습 데이터 전체3. 테스트 데이터1만 개의 MNIST 테스트 데이터 전체
</center>

 학습 진행 과정은 아래와 같이 나타나도록 설정하였다.

![image09](https://github.com/Nov10/nov10.github.io/assets/91333241/27329247-6e5a-4942-8338-ca638a50c194)
<center>[그림 9. 학습 진행 과정]</center><br>

index는 6만 개의 MNIST 학습 데이터 중 현재 학습 중인 데이터의 인덱스[^15]를 나타내고, loss_value는 사전에 설정한 손실 함수(cross-entropy)의 값이다. 해당 값이 8.810 정도에서 시작하여 점점 5.888 정도로 작아지고 있는 것을 볼 수 있다. epochs는 전체 데이터를 순환한 횟수 - 1인데, 1번만 순환하도록 설정하였다.

![image10](https://github.com/Nov10/nov10.github.io/assets/91333241/01884876-7da9-45cd-86f1-c0964d0cd9c1)
<center>[그림 10. 학습 완료 상태]</center><br>

그림 10은 학습이 완료된 상태이다. 6만 개의 모든 MNIST 학습 데이터에 대해서 학습을 진행한 결과 최종 손실함수의 값은 0.746 정도로 나타났으며, 학습에는 18시간 43분 51초가 걸렸다.

MNIST 데이터에 들어있는 1만 개의 테스트 데이터를 이용하여 위에서 학습한 모델의 테스트를 진행했다. 정확도는 92.27%로 나타났다.

최종 학습 결과는 아래와 같다.

<center>
1. 학습 시간<br>
하나의 데이터당 대략 1~2초, 총 18시간 43분 51초<br><br>


2. 최종 결과<br>
손실 함수(cross-entorpy) 값 : 0.7465609...
정확도 : 92.27%
</center>

[^7]: 다른 프로그래머가 만들어둔 코드 집합으로, 행렬 연산 등 여러 가지 편리한 기능들을 제공한다.
[^8]: $y=\frac{1}{1+e^{-x}}$
[^9]: np.exp(-x)는 와 같다.
[^10]: fx1은 이고, fx2는 이고, 2 * delta_x는 이다.
[^11]: $\log(a)$일 $a$때 가 $0$이라면 $\log(a)$는 무한대가 되므로 이를 방지하기 위해 아주 작은 값을 더하여 손실 함수를 계산한다.
[^12]: 정규화란 어떤 데이터들의 값의 범위를 0~1 사이로 조정하는 과정을 뜻한다. 딥러닝에서는 큰 수보다 0~1에서 더 정확도가 높기 때문에 정규화과정을 수행한다.
[^13]: $W'=W-a \frac{\partial E(W,b)}{\partial W}$에서, $a$가 self.learning_rate이고 $\frac{\partial E(W,b)}{\partial W}$가 numerical_derivative(f, self.W2) 또는 numerical_derivative(f, self.W3)이다. 편향도의 경우에도 마찬가지이다.
[^14]: one-hot encoding 방식이란 데이터를 0 혹은 1인 벡터의 원소로 범주화하는 방식이다. 예를 들어, [1, 0, 0]이라는 벡터는 A라는 범주를, [0, 1, 0]이라는 벡터는 B라는 범주를, [0, 0, 1]은 C라는 범주를 뜻하는 방식이 되도록 미리 설정하는 방식이다.
[^15]: 컴퓨터가 순서를 새는 방식이 사람이 순서를 새는 방식과 달라 컴퓨터가 순서를 새는 방식에서 그 ‘순서’를 ‘인덱스(index)’라고 부른다.


Ⅴ. 결론
-------------
지금까지 인공신경망의 개념과 인공신경망과 실제 생물학적 뉴런과 유사점, 차이점에 대해 알아보았다. 이를 바탕으로 인공신경망을 수학적으로 나타내었으며, 이를 이용하여 직접 인공신경망을 구현하여 MNIST 데이터에 대하여 학습을 진행 및 테스트하여 결과를 얻어내었다.

이에 따른 결과로 92.27%의 정확도를 가지는 MNIST 모델을 직접 구축하였다. 92.27% 정도면 높다고 느껴지기보단 오히려 낮은 편이다. 실제 사용 가능한 모델들은 99%에 가까운 정확도를 추구하기 때문에 92.27%라는 수치는 아직 모자란 수치이다.

또한 수치미분 함수는 그 속도가 매우 느리다는 특징에 대해서도 이해하였다. 입력값은 행렬이고, 이 행렬의 크기는 매우 크기 때문에 이에 대한 모든 계산을 모든 데이터에 대해 실행하면서 최적의 가중치와 편향도를 찾아 나가는 과정은 매우 느려 6만 개의 데이터를 한 번씩만 학습했음에도 불구하고 거의 19시간 정도의 시간이 학습에 소요되었다. 학습 시간은 노드의 수에도 비례하는데, 100개 정도의 노드임에도 불구하고 학습이 19시간이 소요되었다면 더 높은 정확도를 위해 층을 두 개 이상 구축하여 여러 개의 노드를 구현하였을 때 이러한 시간을 줄일 방법을 찾는 것이 필연적이라는 것을 보여준다.

그러나 MNIST 모델을 직접 구축하였다는 점에서 그 의의가 있다. 인공지능 제작을 아주 획기적으로 간단하고 접근성을 높이도록 만든 라이브러리인 Tensorflow와 같은 인공지능 관련 외부 라이브러리를 사용하지 않고 수치미분, 경사하강법부터 feed forward 과정까지 전부 하나하나 직접 수학적 수식으로 나타내고 이를 코드로 전부 구현하였다는 점에서 그 의의가 있다.
또한 이러한 모델이 3개의 층으로 이루어진 아주 간단한 인공신경망이면서 더 높은 정확도를 계산하기 위해 다른 손실함수나 최적의 가중치와 편향도를 찾는 방식을 수행하지 않았다는 상황을 고려하면 92.27%라는 정확도 수치는 충분히 만족할 수 있는 수치이다.

이를 통해 지금까지 어렴풋이 알고 있던 인공신경망, 인공지능의 구조와 학습 과정, 최종 데이터 예측까지 수학적으로 이해하고 코드로 직접 처음부터 구현하여 이에 대해 깊이 이해할 수 있었으며, 인공신경망을 이용하면 필기체 인식과 같은 실생활에 사용될 수 있음을 알게 되었다.
또한 모델을 구축하며 수치미분의 느린 속도를 해결하기 위한 오차역전파, 두 개 이상의 은닉층이나 또 다른 구조를 가진 더 복잡하고 정교한 여러 인공신경망 모델에 관해 탐구하고 싶다고 생각하게 되었다.

한편, 교과 ‘생명과학 1’에서 학습했던 생물의 뉴런에 대해서도 인공신경망의 ‘뉴런’인 노드와 비교하며 그 특성을 더 잘 이해할 수 있었다. 노드가 뉴런의 구조를 모방하기 위해 어떠한 방식을 사용하여 공통점을 만들었는지와 층의 존재 여부와 시간에 대한 독립성 등 차이점에 대해서도 더 깊은 이해를 알 수 있었다. 이를 통해 인공신경망이 궁극적으로 인간의 뇌를 모방하기 위한 알고리즘임을 알게 되었으며, 인공신경망뿐만 아니라 실제 뇌와 뉴런을 비롯한 생물의 신경계에 대해서도 그 특징을 더 많이 탐구하고 이를 응용하여 인공신경망 기술에 적용해보고 싶음을 느꼈다.

Ⅵ. 출처 및 참고 자료
-------------
코드 및 데이터

코드는 아래 링크에서 원본을 얻어와 발표자가 직접 수정 및 변형하여 제작함<br>
https://github.com/neowizard2018/neowizard/tree/master/MachineLearning<br>
MNIST 데이터는 아래 링크에서 얻음<br>
https://pjreddie.com/media/files/mnist_train.csv
https://pjreddie.com/media/files/mnist_test.csv

참고 자료

Deep Learning, Wikipedia<br>
https://en.wikipedia.org/wiki/Deep_learning

Artificail Neural Network, Wikipedia<br>
https://en.wikipedia.org/wiki/Artificial_neural_network

Linear Regression, Wikipedia<br>
https://en.wikipedia.org/wiki/Linear_regression

Logistic Regression, Wikipedia<br>
https://en.wikipedia.org/wiki/Logistic_regression

Activation Function, Wikipedia<br>
https://en.wikipedia.org/wiki/Activation_function

Cross-Entropy, Wikipedia<br>
https://en.wikipedia.org/wiki/Cross_entropy

Sigmoid Function, Wikipedia<br>
https://en.wikipedia.org/wiki/Sigmoid_function

Gradient Descent Algorithm<br>
https://www.kaggle.com/code/bhatnagardaksh/gradient-descent-from-scratch

MNIST Database<br>
https://en.wikipedia.org/wiki/MNIST_database


그림

그림 1. 발표자 직접 제작

그림 2. Sigmoid Function<br>
https://en.wikipedia.org/wiki/Sigmoid_function

그림 3. Convolutional Neural Network and Feed Forward Neural Network<br>
https://cloudvane.net/big-data-2/convolutional-neural-network-cnn-and-feedforward-neural-network/

그림 4. Deep Nenural Network<br>
https://ai-diary-by-znreza.com/neural-network-demystified-part-ll

그림 5. 발표자 직접 제작

그림 6. Gradient Descent from scratch<br>
https://www.kaggle.com/code/bhatnagardaksh/gradient-descent-from-scratch

그림 7. MNIST Database<br>
https://pjreddie.com/media/files/mnist_train.csv

그림 8. MNIST Database를 발표자가 직접 캡처함<br>
https://pjreddie.com/media/files/mnist_train.csv

그림 9. 발표자 직접 제작

그림 10. 발표자 직접 제작